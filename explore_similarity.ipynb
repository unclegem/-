{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()\n",
    "from keras.preprocessing.text import one_hot, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.layers.embeddings import *\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 读取所有数据并使格式统一\n",
    "requirement = pd.read_csv('Requirements.csv', header=None, usecols=[0,1,2])\n",
    "requirement.columns = ['Rid', 'R_title', 'R_content']\n",
    "requirement['Rid'] = requirement['Rid'].apply(lambda x: x.replace('\\'', '').strip())\n",
    "\n",
    "train_ach = pd.read_csv('Train_Achievements.csv', usecols=[0,1,2], header=None)\n",
    "train_ach.columns = ['Aid', 'A_title', 'A_content']\n",
    "train_ach['Aid'] = train_ach['Aid'].apply(lambda x: x.replace('\\'', '').strip())\n",
    "\n",
    "test_ach = pd.read_csv('Test_Achievements.csv', usecols=[0,1,2], header=None)\n",
    "test_ach.columns = ['Aid', 'A_title', 'A_content']\n",
    "test_ach['Aid'] = test_ach['Aid'].apply(lambda x: x.replace('\\'', '').strip())\n",
    "\n",
    "train_label = pd.read_csv('Train_Interrelation.csv', usecols=[0,1,2,3])\n",
    "\n",
    "test_pred = pd.read_csv('TestPrediction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_csv('train_features.csv')\n",
    "test_features = pd.read_csv('test_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge\n",
    "train = pd.merge(train_label, requirement, how='left', on='Rid')\n",
    "train = pd.merge(train, train_ach, how='left', on='Aid')\n",
    "\n",
    "test = pd.merge(test_pred, requirement, how='left', on='Rid')\n",
    "test = pd.merge(test, test_ach, how='left', on='Aid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.merge(train, train_features, how='left', on='Guid')\n",
    "test = pd.merge(test, test_features, how='left', on='Guid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清洗文本\n",
    "def clean_line(text):\n",
    "    text = re.sub(\"[A-Za-z0-9\\!\\=\\？\\%\\[\\]\\,\\（\\）\\>\\<:&lt;\\/#\\. -----\\_]\", \"\", text)\n",
    "    text = text.replace('图片', '')\n",
    "    text = text.replace('\\xa0', '') # 删除nbsp\n",
    "    # new\n",
    "    r1 =  \"\\\\【.*?】+|\\\\《.*?》+|\\\\#.*?#+|[.!/_,$&%^*()<>+\"\"'?@|:~{}#]+|[——！\\\\\\，。=？、：“”‘’￥……（）《》【】]\"\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    text = re.sub(cleanr, ' ', text)        #去除html标签\n",
    "    text = re.sub(r1,'',text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "stop_words = pd.read_table('stop.txt', header=None)[0].tolist()\n",
    "\n",
    "cols = ['A_title', 'A_content', 'R_title', 'R_content']\n",
    "for col in cols:\n",
    "    train[col] = train[col].apply(lambda x: clean_line(x))\n",
    "    test[col] = test[col].apply(lambda x: clean_line(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[(train.len_A_title != 0) & (train.len_R_title != 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "garbage = test[(test.len_A_title == 0) | (test.len_R_title == 0)][['Guid', 'Level']]\n",
    "garbage.Level = 1\n",
    "\n",
    "test_need = test[(test.len_A_title != 0) & (test.len_R_title != 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 中文分词\n",
    "import jieba\n",
    "import string\n",
    "def cut_text(sentence):\n",
    "    tokens = list(jieba.cut(sentence))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    return tokens\n",
    "jieba.load_userdict('./itwords.txt')\n",
    "jieba.load_userdict('./ecowords.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_R_title = [cut_text(sent) for sent in train.R_title.values]\n",
    "train_R_content = [cut_text(sent) for sent in train.R_content.values]\n",
    "train_A_title = [cut_text(sent) for sent in train.A_title.values]\n",
    "train_A_content = [cut_text(sent) for sent in train.A_content.values]\n",
    "\n",
    "test_R_title = [cut_text(sent) for sent in test_need.R_title.values]\n",
    "test_R_content = [cut_text(sent) for sent in test_need.R_content.values]\n",
    "test_A_title = [cut_text(sent) for sent in test_need.A_title.values]\n",
    "test_A_content = [cut_text(sent) for sent in test_need.A_content.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练w2v\n",
    "all_doc = train_R_content + train_R_title + train_A_content + train_A_title + \\\n",
    "          test_R_content + test_R_title + test_A_content + test_A_title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import time\n",
    "class EpochSaver(gensim.models.callbacks.CallbackAny2Vec):\n",
    "    '''用于保存模型, 打印损失函数等等'''\n",
    "    def __init__(self, save_path):\n",
    "        self.save_path = save_path\n",
    "        self.epoch = 0\n",
    "        self.pre_loss = 0\n",
    "        self.best_loss = 999999999.9\n",
    "        self.since = time.time()\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        self.epoch += 1\n",
    "        cum_loss = model.get_latest_training_loss() # 返回的是从第一个epoch累计的\n",
    "        epoch_loss = cum_loss - self.pre_loss\n",
    "        time_taken = time.time() - self.since\n",
    "        print(\"Epoch %d, loss: %.2f, time: %dmin %ds\" % \n",
    "                    (self.epoch, epoch_loss, time_taken//60, time_taken%60))\n",
    "        if self.best_loss > epoch_loss:\n",
    "            self.best_loss = epoch_loss\n",
    "            print(\"Better model. Best loss: %.2f\" % self.best_loss)\n",
    "            model.save(self.save_path)\n",
    "            print(\"Model %s save done!\" % self.save_path)\n",
    "\n",
    "        self.pre_loss = cum_loss\n",
    "        self.since = time.time()\n",
    "model_word2vec = gensim.models.Word2Vec.load('final_word2vec_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_word2vec = gensim.models.Word2Vec(min_count=1, \n",
    "                                        window=5, \n",
    "                                        size=256,\n",
    "                                        workers=4,\n",
    "                                        batch_words=1000)\n",
    "since = time.time()\n",
    "model_word2vec.build_vocab(all_doc, progress_per=2000)\n",
    "time_elapsed = time.time() - since\n",
    "print('Time to build vocab: {:.0f}min {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "since = time.time()\n",
    "model_word2vec.train(all_doc, total_examples=model_word2vec.corpus_count, \n",
    "                        epochs=5, compute_loss=True, report_delay=60*10, # 每隔10分钟输出一下日志\n",
    "                        callbacks=[EpochSaver('./final_word2vec_model')])\n",
    "time_elapsed = time.time() - since\n",
    "print('Time to train: {:.0f}min {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取title + content 128len\n",
    "# train_A_doc = [t + c for t, c in zip(train_A_title, train_A_content)]\n",
    "# test_A_doc = [t + c for t, c in zip(test_A_title, test_A_content)]\n",
    "# train_R_doc = [t + c for t, c in zip(train_R_title, train_R_content)]\n",
    "# test_R_doc = [t + c for t, c in zip(test_R_title, test_R_content)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本序列化。\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_A_title + test_A_title + train_R_title + test_R_title)\n",
    "# tokenizer.fit_on_texts(train_A_doc + test_A_doc + train_R_doc + test_R_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转化成词向量矩阵，利用新的word2vec模型\n",
    "vocab_size = len(tokenizer.word_index)\n",
    "error_count=0\n",
    "embedding_matrix = np.zeros((vocab_size + 1, 256))\n",
    "for word, i in tqdm(tokenizer.word_index.items()):\n",
    "    if word in model_word2vec:\n",
    "        embedding_matrix[i] = model_word2vec.wv[word]\n",
    "    else:\n",
    "        error_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 30\n",
    "\n",
    "sequence = tokenizer.texts_to_sequences(train_A_title)\n",
    "train_query = pad_sequences(sequence, maxlen=max_len)\n",
    "sequence = tokenizer.texts_to_sequences(train_R_title)\n",
    "train_title = pad_sequences(sequence, maxlen=max_len)\n",
    "\n",
    "sequence = tokenizer.texts_to_sequences(test_A_title)\n",
    "test_query = pad_sequences(sequence, maxlen=max_len)\n",
    "sequence = tokenizer.texts_to_sequences(test_R_title)\n",
    "test_title = pad_sequences(sequence, maxlen=max_len)\n",
    "\n",
    "# sequence = tokenizer.texts_to_sequences(train_A_doc)\n",
    "# train_query = pad_sequences(sequence, maxlen=max_len)\n",
    "# sequence = tokenizer.texts_to_sequences(train_R_doc)\n",
    "# train_title = pad_sequences(sequence, maxlen=max_len)\n",
    "\n",
    "# sequence = tokenizer.texts_to_sequences(test_A_doc)\n",
    "# test_query = pad_sequences(sequence, maxlen=max_len)\n",
    "# sequence = tokenizer.texts_to_sequences(test_R_doc)\n",
    "# test_title = pad_sequences(sequence, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['word_match', 'jaccard', 'common_words',\n",
    "       'total_unique_words', 'wc_diff', 'wc_ratio', 'wc_diff_unique',\n",
    "       'wc_ratio_unique', 'same_start_word', 'tfidf_wm', 'query_length',\n",
    "       'title_length', 'query_isin_title', 'len_A_title', 'len_R_title',\n",
    "       'diff_len', 'fuzz_qratio', 'fuzz_WRatio', 'fuzz_partial_ratio',\n",
    "       'fuzz_partial_token_set_ratio', 'fuzz_partial_token_sort_ratio',\n",
    "       'fuzz_token_set_ratio', 'fuzz_token_sort_ratio', 'wmd',\n",
    "       'cosine_distance', 'cityblock_distance', 'canberra_distance',\n",
    "       'euclidean_distance', 'minkowski_distance', 'braycurtis_distance',\n",
    "       'skew_q1vec', 'skew_q2vec', 'kur_q1vec', 'kur_q2vec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "import keras\n",
    "def model_conv1D_(emb_matrix, max_len):\n",
    "    # The embedding layer containing the word vectors\n",
    "    emb_layer = Embedding(\n",
    "        input_dim=emb_matrix.shape[0],\n",
    "        output_dim=emb_matrix.shape[1],\n",
    "        weights=[emb_matrix],\n",
    "        input_length=max_len,\n",
    "        trainable=False\n",
    "    )\n",
    "    \n",
    "    # 1D convolutions that can iterate over the word vectors\n",
    "    conv1 = Conv1D(filters=128, kernel_size=1, padding='same', activation='relu')\n",
    "    conv2 = Conv1D(filters=128, kernel_size=2, padding='same', activation='relu')\n",
    "    conv3 = Conv1D(filters=128, kernel_size=3, padding='same', activation='relu')\n",
    "    conv4 = Conv1D(filters=128, kernel_size=4, padding='same', activation='relu')\n",
    "    conv5 = Conv1D(filters=32, kernel_size=5, padding='same', activation='relu')\n",
    "    conv6 = Conv1D(filters=32, kernel_size=6, padding='same', activation='relu')\n",
    "\n",
    "    # Define inputs\n",
    "    seq1 = Input(shape=(max_len,))\n",
    "    seq2 = Input(shape=(max_len,))\n",
    "\n",
    "    # Run inputs through embedding\n",
    "    emb1 = emb_layer(seq1)\n",
    "    emb2 = emb_layer(seq2)\n",
    "\n",
    "    # Run through CONV + GAP layers\n",
    "    conv1a = conv1(emb1)\n",
    "    glob1a = GlobalAveragePooling1D()(conv1a)\n",
    "    conv1b = conv1(emb2)\n",
    "    glob1b = GlobalAveragePooling1D()(conv1b)\n",
    "\n",
    "    conv2a = conv2(emb1)\n",
    "    glob2a = GlobalAveragePooling1D()(conv2a)\n",
    "    conv2b = conv2(emb2)\n",
    "    glob2b = GlobalAveragePooling1D()(conv2b)\n",
    "\n",
    "    conv3a = conv3(emb1)\n",
    "    glob3a = GlobalAveragePooling1D()(conv3a)\n",
    "    conv3b = conv3(emb2)\n",
    "    glob3b = GlobalAveragePooling1D()(conv3b)\n",
    "\n",
    "    conv4a = conv4(emb1)\n",
    "    glob4a = GlobalAveragePooling1D()(conv4a)\n",
    "    conv4b = conv4(emb2)\n",
    "    glob4b = GlobalAveragePooling1D()(conv4b)\n",
    "\n",
    "    conv5a = conv5(emb1)\n",
    "    glob5a = GlobalAveragePooling1D()(conv5a)\n",
    "    conv5b = conv5(emb2)\n",
    "    glob5b = GlobalAveragePooling1D()(conv5b)\n",
    "\n",
    "    conv6a = conv6(emb1)\n",
    "    glob6a = GlobalAveragePooling1D()(conv6a)\n",
    "    conv6b = conv6(emb2)\n",
    "    glob6b = GlobalAveragePooling1D()(conv6b)\n",
    "\n",
    "    mergea = concatenate([glob1a, glob2a, glob3a, glob4a, glob5a, glob6a])\n",
    "    mergeb = concatenate([glob1b, glob2b, glob3b, glob4b, glob5b, glob6b])\n",
    "\n",
    "    # We take the explicit absolute difference between the two sentences\n",
    "    # Furthermore we take the multiply different entries to get a different measure of equalness\n",
    "    diff = Lambda(lambda x: K.abs(x[0] - x[1]), output_shape=(4 * 128 + 2*32,))([mergea, mergeb])\n",
    "    mul = Lambda(lambda x: x[0] * x[1], output_shape=(4 * 128 + 2*32,))([mergea, mergeb])\n",
    "\n",
    "    # Add the magic features\n",
    "    # magic_input = Input(shape=(5,))\n",
    "    # magic_dense = BatchNormalization()(magic_input)\n",
    "    # magic_dense = Dense(64, activation='relu')(magic_dense)\n",
    "\n",
    "    # Add the distance features (these are now TFIDF (character and word), Fuzzy matching, \n",
    "    # nb char 1 and 2, word mover distance and skew/kurtosis of the sentence vector)\n",
    "    \n",
    "    distance_input = Input(shape=(34,))\n",
    "    distance_dense = BatchNormalization()(distance_input)\n",
    "    distance_dense = Dense(128, activation='relu')(distance_dense)\n",
    "\n",
    "    # Merge the Magic and distance features with the difference layer\n",
    "    merge = concatenate([diff, mul, distance_dense])\n",
    "#     merge = concatenate([diff, mul])\n",
    "    # merge = concatenate([diff, mul, magic_dense, distance_dense])\n",
    "\n",
    "\n",
    "    # The MLP that determines the outcome\n",
    "    x = Dropout(0.2)(merge)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(300, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    pred = Dense(4, activation='softmax')(x)\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam(lr=0.005)\n",
    "    model = Model(inputs=[seq1, seq2, distance_input], outputs=pred)\n",
    "#     model = Model(inputs=[seq1, seq2], outputs=pred)\n",
    "    # model = Model(inputs=[seq1, seq2, magic_input, distance_input], outputs=pred)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['acc'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = model_conv1D_(embedding_matrix, max_len)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = np.array(train.Level.tolist()) - 1\n",
    "train_query_train, train_query_val = train_query[:6000], train_query[6000:]\n",
    "train_title_train, train_title_val = train_title[:6000], train_title[6000:]\n",
    "label_train, label_val = label[:6000], label[6000:]\n",
    "train_features_train, train_features_val = train[:6000][features], train[6000:][features]\n",
    "label_train = to_categorical(label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model.fit([train_query_train, train_title_train, train_features_train],         \n",
    "#           label_train,\n",
    "#           batch_size = 64,\n",
    "#           epochs=5,\n",
    "#           shuffle=True,\n",
    "#           )\n",
    "model.fit([train_query, train_title, train[features]],         \n",
    "          to_categorical(label),\n",
    "          batch_size = 128,\n",
    "          epochs=15,\n",
    "          shuffle=True,\n",
    "          )\n",
    "# 25 good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "pred_val = model.predict([train_query_val, train_title_val, train_features_val])\n",
    "print(f1_score(label_val, np.argmax(pred_val, axis=1), average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.argmax(model.predict([test_query, test_title, test_need[features]]), axis=1) + 1\n",
    "test_need['Level'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pred = pd.concat([garbage, test_need[['Guid', 'Level']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.drop('Level', axis=1)\n",
    "test = pd.merge(test, all_pred, how='left', on='Guid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.Level.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test[['Guid', 'Level']].to_csv('./subs/baseline8_add_features.csv', header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('./subs/baseline2.csv', header=None)[1].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
