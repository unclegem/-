{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()\n",
    "from keras.preprocessing.text import one_hot, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.layers.embeddings import *\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# 读取所有数据并使格式统一\n",
    "requirement = pd.read_csv('Requirements.csv', header=None, usecols=[0,1,2])\n",
    "requirement.columns = ['Rid', 'R_title', 'R_content']\n",
    "requirement['Rid'] = requirement['Rid'].apply(lambda x: x.replace('\\'', '').strip())\n",
    "\n",
    "train_ach = pd.read_csv('Train_Achievements.csv', usecols=[0,1,2], header=None)\n",
    "train_ach.columns = ['Aid', 'A_title', 'A_content']\n",
    "train_ach['Aid'] = train_ach['Aid'].apply(lambda x: x.replace('\\'', '').strip())\n",
    "\n",
    "test_ach = pd.read_csv('Test_Achievements.csv', usecols=[0,1,2], header=None)\n",
    "test_ach.columns = ['Aid', 'A_title', 'A_content']\n",
    "test_ach['Aid'] = test_ach['Aid'].apply(lambda x: x.replace('\\'', '').strip())\n",
    "\n",
    "train_label = pd.read_csv('Train_Interrelation.csv', usecols=[0,1,2,3])\n",
    "\n",
    "test_pred = pd.read_csv('TestPrediction.csv')\n",
    "\n",
    "# merge\n",
    "train = pd.merge(train_label, requirement, how='left', on='Rid')\n",
    "train = pd.merge(train, train_ach, how='left', on='Aid')\n",
    "\n",
    "test = pd.merge(test_pred, requirement, how='left', on='Rid')\n",
    "test = pd.merge(test, test_ach, how='left', on='Aid')\n",
    "\n",
    "# 清洗文本\n",
    "def clean_line(text):\n",
    "    text = re.sub(\"[A-Za-z0-9\\!\\=\\？\\%\\[\\]\\,\\（\\）\\>\\<:&lt;\\/#\\. -----\\_]\", \"\", text)\n",
    "    text = text.replace('图片', '')\n",
    "    text = text.replace('\\xa0', '') # 删除nbsp\n",
    "    # new\n",
    "    r1 =  \"\\\\【.*?】+|\\\\《.*?》+|\\\\#.*?#+|[.!/_,$&%^*()<>+\"\"'?@|:~{}#]+|[——！\\\\\\，。=？、：“”‘’￥……（）《》【】]\"\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    text = re.sub(cleanr, ' ', text)        #去除html标签\n",
    "    text = re.sub(r1,'',text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "stop_words = pd.read_table('stop.txt', header=None)[0].tolist()\n",
    "\n",
    "cols = ['A_title', 'A_content', 'R_title', 'R_content']\n",
    "for col in cols:\n",
    "    train[col] = train[col].apply(lambda x: clean_line(x))\n",
    "    test[col] = test[col].apply(lambda x: clean_line(x))\n",
    "    \n",
    "# 中文分词\n",
    "import jieba\n",
    "import string\n",
    "def cut_text(sentence):\n",
    "    tokens = list(jieba.cut(sentence))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    return tokens\n",
    "jieba.load_userdict('./itwords.txt')\n",
    "jieba.load_userdict('./ecowords.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_match_share(row):\n",
    "    query = cut_text(row['A_title'])\n",
    "    title = cut_text(row['R_title'])\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in query:\n",
    "        q1words[word] = 1\n",
    "    for word in title:\n",
    "        q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
    "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
    "    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n",
    "    return R\n",
    "\n",
    "def jaccard(row):\n",
    "    query = set(cut_text(row['A_title']))\n",
    "    title = set(cut_text(row['R_title']))\n",
    "    wic = query.intersection(title)\n",
    "    uw = query.union(title)\n",
    "    if len(uw) == 0:\n",
    "        uw = [1]\n",
    "    return (len(wic) / len(uw))\n",
    "\n",
    "def common_words(row):\n",
    "    query = set(cut_text(row['A_title']))\n",
    "    title = set(cut_text(row['R_title']))\n",
    "    return len(set(query).intersection(set(title)))\n",
    "\n",
    "def total_unique_words(row):\n",
    "    query = set(cut_text(row['A_title']))\n",
    "    title = set(cut_text(row['R_title']))\n",
    "    return len(set(query).union(title))\n",
    "\n",
    "def wc_diff(row):\n",
    "    query = cut_text(row['A_title'])\n",
    "    title = cut_text(row['R_title'])\n",
    "    return abs(len(query) - len(title))\n",
    "\n",
    "def wc_ratio(row):\n",
    "    query = cut_text(row['A_title'])\n",
    "    title = cut_text(row['R_title'])\n",
    "    l1 = len(query)*1.0 \n",
    "    l2 = len(title)\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "\n",
    "def wc_diff_unique(row):\n",
    "    query = cut_text(row['A_title'])\n",
    "    title = cut_text(row['R_title'])\n",
    "    return abs(len(set(query)) - len(set(title)))\n",
    "\n",
    "def wc_ratio_unique(row):\n",
    "    query = cut_text(row['A_title'])\n",
    "    title = cut_text(row['R_title'])\n",
    "    l1 = len(set(query)) * 1.0\n",
    "    l2 = len(set(title))\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "\n",
    "def same_start_word(row):\n",
    "    query = cut_text(row['A_title'])\n",
    "    title = cut_text(row['R_title'])\n",
    "    if not query or not title:\n",
    "        return np.nan\n",
    "    return int(query[0] == title[0])\n",
    "\n",
    "def get_weight(count, eps=10000, min_count=2):\n",
    "    query = row['query'].replace('\\t', '').split()\n",
    "    title = row['title'].replace('\\t', '').split()\n",
    "    if count < min_count:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 / (count + eps)\n",
    "\n",
    "def tfidf_word_match_share(row, weights=None):\n",
    "    query = cut_text(row['A_title'])\n",
    "    title = cut_text(row['R_title'])\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in query:\n",
    "        q1words[word] = 1\n",
    "    for word in title:\n",
    "        q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    \n",
    "    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n",
    "    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n",
    "    \n",
    "    R = np.sum(shared_weights) / np.sum(total_weights)\n",
    "    return R\n",
    "\n",
    "# tfidf 所需\n",
    "from collections import Counter\n",
    "train_qs = pd.Series(train['A_title'].apply(lambda x: cut_text(x)).tolist() \n",
    "                    + train['R_title'].apply(lambda x: cut_text(x)).tolist()\n",
    "                    + test['A_title'].apply(lambda x: cut_text(x)).tolist() \n",
    "                    + test['R_title'].apply(lambda x: cut_text(x)).tolist())\n",
    "words = [x for y in train_qs for x in y]\n",
    "counts = Counter(words)\n",
    "\n",
    "def get_weight(count, eps=10000, min_count=2):\n",
    "    if count < min_count:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 / (count + eps)\n",
    "weights = {word: get_weight(count) for word, count in counts.items()}\n",
    "\n",
    "def get_statistic_features(train):\n",
    "    # 14 dimensions\n",
    "    train['word_match'] = train.apply(word_match_share, axis=1) #1\n",
    "    train['jaccard'] = train.apply(jaccard, axis=1, raw=True) #2\n",
    "    train['common_words'] = train.apply(common_words, axis=1, raw=True) #3\n",
    "    train['total_unique_words'] = train.apply(total_unique_words, axis=1, raw=True) #4\n",
    "    train['wc_diff'] = train.apply(wc_diff, axis=1, raw=True) #5\n",
    "    train['wc_ratio'] = train.apply(wc_ratio, axis=1, raw=True) #6\n",
    "    train['wc_diff_unique'] = train.apply(wc_diff_unique, axis=1, raw=True) #7\n",
    "    train['wc_ratio_unique'] = train.apply(wc_ratio_unique, axis=1, raw=True) #8\n",
    "    train['same_start_word'] = train.apply(same_start_word, axis=1, raw=True) #9\n",
    "    train['tfidf_wm'] = train.apply(lambda x: tfidf_word_match_share(x, weights), axis=1, raw=True) #11\n",
    "    train['query_length'] = train['A_title'].apply(lambda x: len(cut_text(x))) #12\n",
    "    train['title_length'] = train['R_title'].apply(lambda x: len(cut_text(x))) #13\n",
    "    train['query_isin_title'] = train.apply(lambda row:1 if row['A_title'] in row['R_title'] else 0, axis = 1) #14\n",
    "    return train\n",
    "train = get_statistic_features(train)\n",
    "test = get_statistic_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import time\n",
    "class EpochSaver(gensim.models.callbacks.CallbackAny2Vec):\n",
    "    '''用于保存模型, 打印损失函数等等'''\n",
    "    def __init__(self, save_path):\n",
    "        self.save_path = save_path\n",
    "        self.epoch = 0\n",
    "        self.pre_loss = 0\n",
    "        self.best_loss = 999999999.9\n",
    "        self.since = time.time()\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        self.epoch += 1\n",
    "        cum_loss = model.get_latest_training_loss() # 返回的是从第一个epoch累计的\n",
    "        epoch_loss = cum_loss - self.pre_loss\n",
    "        time_taken = time.time() - self.since\n",
    "        print(\"Epoch %d, loss: %.2f, time: %dmin %ds\" % \n",
    "                    (self.epoch, epoch_loss, time_taken//60, time_taken%60))\n",
    "        if self.best_loss > epoch_loss:\n",
    "            self.best_loss = epoch_loss\n",
    "            print(\"Better model. Best loss: %.2f\" % self.best_loss)\n",
    "            model.save(self.save_path)\n",
    "            print(\"Model %s save done!\" % self.save_path)\n",
    "\n",
    "        self.pre_loss = cum_loss\n",
    "        self.since = time.time()\n",
    "model = gensim.models.Word2Vec.load('./final_word2vec_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.spatial.distance import cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis\n",
    "\n",
    "def wmd(s1, s2, model):\n",
    "        model = model\n",
    "        s1 = cut_text(s1)\n",
    "        s2 = cut_text(s2)\n",
    "        return model.wv.wmdistance(s1, s2)\n",
    "\n",
    "def sent2vec(s, model):\n",
    "    model = model\n",
    "    words = cut_text(s)\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(model[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    return v / np.sqrt((v ** 2).sum())\n",
    "\n",
    "def get_other_features(data):\n",
    "    print('---basic features begin---')   \n",
    "    # 15 dimensions\n",
    "    data['len_A_title'] = data[\"A_title\"].apply(lambda x: len(str(x)))\n",
    "    data['len_R_title'] = data[\"R_title\"].apply(lambda x: len(str(x)))\n",
    "    data['diff_len'] = data.len_A_title - data.len_R_title\n",
    "    data['fuzz_qratio'] = data.apply(lambda x: fuzz.QRatio(str(x['A_title']), str(x['R_title'])), axis=1)\n",
    "    data['fuzz_WRatio'] = data.apply(lambda x: fuzz.WRatio(str(x['A_title']), str(x['R_title'])), axis=1)\n",
    "    data['fuzz_partial_ratio'] = data.apply(lambda x: fuzz.partial_ratio(str(x['A_title']), str(x['R_title'])), axis=1)\n",
    "    data['fuzz_partial_token_set_ratio'] = data.apply(lambda x: fuzz.partial_token_set_ratio(str(x['A_title']), str(x['R_title'])), axis=1)\n",
    "    data['fuzz_partial_token_sort_ratio'] = data.apply(lambda x: fuzz.partial_token_sort_ratio(str(x['A_title']), str(x['R_title'])), axis=1)\n",
    "    data['fuzz_token_set_ratio'] = data.apply(lambda x: fuzz.token_set_ratio(str(x['A_title']), str(x['R_title'])), axis=1)\n",
    "    data['fuzz_token_sort_ratio'] = data.apply(lambda x: fuzz.token_sort_ratio(str(x['A_title']), str(x['R_title'])), axis=1)\n",
    "    print('---basic features finished---')\n",
    "    print('---wmd features begin---')\n",
    "    data['wmd'] = data.apply(lambda x: wmd(x['A_title'], x['R_title'], model), axis=1)\n",
    "    print('---wmd features finished---')\n",
    "\n",
    "    print('---sent2vec begin---')\n",
    "    # 提取的词向量维度为256\n",
    "    question1_vectors = np.zeros((data.shape[0], 256))\n",
    "    error_count = 0\n",
    "\n",
    "    for i, q in tqdm(enumerate(data[\"A_title\"].values)):\n",
    "        question1_vectors[i, :] = sent2vec(q, model)\n",
    "\n",
    "    question2_vectors  = np.zeros((data.shape[0], 256))\n",
    "    for i, q in tqdm(enumerate(data[\"R_title\"].values)):\n",
    "        question2_vectors[i, :] = sent2vec(q, model)\n",
    "    print('---sent2vec finished---')\n",
    "\n",
    "    print('---distance features begin---')\n",
    "    data['cosine_distance'] = [cosine(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                              np.nan_to_num(question2_vectors))]\n",
    "    data['cityblock_distance'] = [cityblock(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                              np.nan_to_num(question2_vectors))]\n",
    "    data['canberra_distance'] = [canberra(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                              np.nan_to_num(question2_vectors))]\n",
    "    data['euclidean_distance'] = [euclidean(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                              np.nan_to_num(question2_vectors))]\n",
    "    data['minkowski_distance'] = [minkowski(x, y, 3) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                              np.nan_to_num(question2_vectors))]\n",
    "    data['braycurtis_distance'] = [braycurtis(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                              np.nan_to_num(question2_vectors))]\n",
    "    print('---distance features finished---')\n",
    "\n",
    "    print('---skew_kur features begin---')\n",
    "    data['skew_q1vec'] = [skew(x) for x in np.nan_to_num(question1_vectors)]\n",
    "    data['skew_q2vec'] = [skew(x) for x in np.nan_to_num(question2_vectors)]\n",
    "    data['kur_q1vec'] = [kurtosis(x) for x in np.nan_to_num(question1_vectors)]\n",
    "    data['kur_q2vec'] = [kurtosis(x) for x in np.nan_to_num(question2_vectors)]\n",
    "    print('---skew_kur features finished---')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = get_other_features(train)\n",
    "test = get_other_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_fea = ['Aid', 'Rid', 'R_title', 'A_title', 'A_content', 'R_content', 'Level']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(drop_fea, axis=1).to_csv('train_features.csv', index=False)\n",
    "test.drop(drop_fea, axis=1).to_csv('test_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
